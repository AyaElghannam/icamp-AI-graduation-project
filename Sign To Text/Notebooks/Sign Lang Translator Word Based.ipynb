{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time , random\n",
    "import mediapipe as mp\n",
    "import copy\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense,Dropout,BatchNormalization,Input,Conv1D,MaxPooling1D,\\\n",
    "                                    TimeDistributed,Activation,Lambda,ReLU,Conv1D,ConvLSTM1D,Flatten\n",
    "from tensorflow.keras.callbacks import TensorBoard,ModelCheckpoint,EarlyStopping\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score,confusion_matrix\n",
    "from tensorflow.keras.optimizers import Adam,RMSprop\n",
    "from tensorflow.python.client import device_lib \n",
    "from tensorflow.keras.utils import plot_model\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU Existing\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Keypoints using MP Holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    \"\"\"\n",
    "    inputs: CV2 Image\n",
    "    output: Image, detected Landmarks\n",
    "    \"\"\"\n",
    "    \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable to Improve Perf.\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw Landmarks on Image\n",
    "def draw_landmarks(image, results):\n",
    "    \"\"\"\n",
    "    inputs: CV2 Image , Landmarks[Model Results]\n",
    "    \"\"\"\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION) # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw Styled Landmarks on Image\n",
    "def draw_styled_landmarks(image, results):\n",
    "    \"\"\"\n",
    "    inputs: CV2 Image , Landmarks[Model Results]\n",
    "    \"\"\"\n",
    "    # Draw face connections\n",
    "\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, \n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                             ) \n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Extract Keypoint Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Keypoints from Landmarks And Concatenate it in One Array\n",
    "def extract_keypoints(results):\n",
    "    \"\"\"\n",
    "    inputs: Resutls from MediaPipe Model\n",
    "    output: Concatenated Landmarks\n",
    "    \"\"\"\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Setup Folders for Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our Actions\n",
    "actions = np.array(['Hello', 'I', 'How are you', 'Brother','Sister','Father','Mother','Egypt','Front of'])\n",
    "# DataPath of Data\n",
    "DATA_PATH_KEYPOINTS = os.path.join('AllData')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Preprocess Data and Create Labels and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labeling Actions to Numbers\n",
    "label_map = {label:num for num, label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load All Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data 60 fps then Transform it to 30fps\n",
    "sequences1, labels1 = [], []\n",
    "DATA_PATH_KEYPOINTS = os.path.join('AllData')\n",
    "for action in actions:\n",
    "    list_seq = glob.glob('/'.join([DATA_PATH_KEYPOINTS,'60FPS',action,'*']))\n",
    "    for sequence in list_seq:       \n",
    "        window = []\n",
    "        # Take two Steps to Convert it To 30FPS\n",
    "        for frame_num in range(0,60,2):\n",
    "            #Load Frame Keypoints\n",
    "            res1 = np.load(os.path.join(sequence, \"{}.npy\".format(frame_num)))\n",
    "            #Take Hands Landmarks\n",
    "            lh_rh = res1[1536:]\n",
    "            #Remove z Axis From Landmarks\n",
    "            for z in range(2,lh_rh.shape[0],3):\n",
    "                    lh_rh[z] = None\n",
    "            #Remove NaN Data\n",
    "            lh_rh = lh_rh[np.logical_not(np.isnan(lh_rh))]\n",
    "            window.append(lh_rh)\n",
    "        sequences1.append(window)\n",
    "        labels1.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert Lists To Array\n",
    "X2 = np.array(sequences1)\n",
    "#Convert Labels to OHE\n",
    "y2 = to_categorical(labels1).astype(int)\n",
    "X2.shape, y2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 30 Fps Data Folder \n",
    "DATA_PATH_KEYPOINTS = os.path.join('AllData')\n",
    "sequences, labels = [], []\n",
    "for action in actions:\n",
    "    list_seq = glob.glob('/'.join([DATA_PATH_KEYPOINTS,'30FPS',action,'*']))\n",
    "    for sequence in list_seq:       \n",
    "        window = []\n",
    "        for frame_num in range(0,30):\n",
    "            #Load Frame Keypoints\n",
    "            res1 = np.load(os.path.join(str(sequence), \"{}.npy\".format(frame_num)))\n",
    "            #Take Hands Landmarks\n",
    "            lh_rh = res1[1536:]\n",
    "            #Remove z Axis From Landmarks\n",
    "            for z in range(2,lh_rh.shape[0],3):\n",
    "                    lh_rh[z] = None\n",
    "            #Remove NaN Data\n",
    "            lh_rh = lh_rh[np.logical_not(np.isnan(lh_rh))]\n",
    "            window.append(lh_rh)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert Lists To Array\n",
    "X1 = np.array(sequences)\n",
    "#Convert Labels to OHE\n",
    "y1 = to_categorical(labels).astype(int)\n",
    "X1.shape, y1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all data\n",
    "X = np.concatenate([X1,X2])\n",
    "y = np.concatenate([y1,y2])\n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotation Augmentation\n",
    "def augment_data_rotataion(X,y):\n",
    "    '''\n",
    "    input: X,y  as numpy array Shape: [Samples,Timesteps,Features]\n",
    "    output: Augmented X,y as numpy array Shape:[Samples,Timesteps,Features]\n",
    "    '''\n",
    "    # Make an Array with Shape Like Original One\n",
    "    augmented_X = np.zeros_like(X)\n",
    "    augmented_y = np.zeros_like(y)\n",
    "    \n",
    "    #Looping in all Examples\n",
    "    for ex in range(X.shape[0]):\n",
    "        # Get Random Angle Betwwen -5,5\n",
    "        rotation_angle = random.randint(-5,5)\n",
    "        # Convert it to Radians\n",
    "        theta = np.radians(rotation_angle)\n",
    "        c, s = np.cos(theta), np.sin(theta)\n",
    "        # Build a Rotation Matrix\n",
    "        rotation_matrix = np.array(((c, -s), (s, c)))\n",
    "        # Looping Each Frame\n",
    "        for frame in range(X.shape[1]):\n",
    "            window = []\n",
    "            # looping each Point within Frame\n",
    "            for i in range(0,X.shape[2]-1,2):\n",
    "                # Get Keypoint\n",
    "                keypoint = np.array([X[ex][frame][i],X[ex][frame][i+1]])\n",
    "                # Calculate Rotated Keypoint\n",
    "                rotated_keypoint = np.dot(rotation_matrix, keypoint)\n",
    "                keypoint_x = rotated_keypoint[0]\n",
    "                keypoint_y = rotated_keypoint[1]\n",
    "                # Append New Keypoint To our Data\n",
    "                window.extend([keypoint_x,keypoint_y])\n",
    "            augmented_X[ex][frame] = np.array(window)\n",
    "        augmented_y[ex] = y[ex]\n",
    "    return augmented_X,augmented_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale Augmentation\n",
    "def augment_data_scale(X,y):\n",
    "    '''\n",
    "    input: X,y  as numpy array Shape: [Samples,Timesteps,Features]\n",
    "    output: Augmented X,y as numpy array Shape: [Samples,Timesteps,Features]\n",
    "    '''\n",
    "    # Make an Array with Shape Like Original One\n",
    "    augmented_X = np.zeros_like(X)\n",
    "    augmented_y = np.zeros_like(y)\n",
    "    # Looping in Each Sample\n",
    "    for ex in range(X.shape[0]):\n",
    "        # Get Random Scale Factor\n",
    "        SCALE = round(random.random(),2)\n",
    "        for frame in range(X.shape[1]):\n",
    "            # Calculate New Point\n",
    "            augmented_X[ex][frame] = X[ex][frame]*SCALE\n",
    "        augmented_y[ex] = y[ex]\n",
    "    return augmented_X,augmented_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmented Rotated Data\n",
    "rot_x,rot_y = augment_data_rotataion(X,y) \n",
    "# Augmented Scaled Data\n",
    "scaled_x,scaled_y = augment_data_scale(X,y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all data [Original and Augmented]\n",
    "X_ = np.concatenate([X,rot_x,scaled_x])\n",
    "y_ = np.concatenate([y,rot_y,scaled_y])\n",
    "X_.shape,y_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_, y_, test_size=0.20,shuffle=True,stratify=y_,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Build and Train Neural Network Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define The Time Steps\n",
    "timesteps = X.shape[1]\n",
    "# Define The Features No.\n",
    "features = X.shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build a LSTM Model Arch\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(timesteps,features))) # frames * Features\n",
    "model_lstm.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model_lstm.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "model_lstm.add(Dense(64, activation='relu'))\n",
    "model_lstm.add(Dense(32, activation='relu'))\n",
    "model_lstm.add(Dense(actions.shape[0], activation='softmax'))\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilation Configuration\n",
    "model_lstm.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Callbacks\n",
    "log_dir = os.path.join('Logs/LSTM3')\n",
    "tb_callback = TensorBoard(log_dir=log_dir,histogram_freq=1,\n",
    "                          update_freq='epoch',\n",
    "                          profile_batch=0) ## !tensorboard --logdir=.\n",
    "mc = ModelCheckpoint('Models/LSTM3.h5', monitor='val_categorical_accuracy', mode='max', verbose=1,save_best_only=True)\n",
    "es = EarlyStopping(monitor='val_categorical_accuracy', mode='max', verbose=1,patience=100)\n",
    "callbacks = [tb_callback,mc,es]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm.fit(X_train, y_train, epochs=2000, callbacks=[callbacks],batch_size=32,validation_data=(X_test,y_test),initial_epoch = 13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Conv1d + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_convlstm = Sequential()\n",
    "\n",
    "model_convlstm.add(Input(shape=(timesteps,features))) \n",
    "model_convlstm.add(Conv1D(128, # Filters\n",
    "                 5, # Kernel Size\n",
    "                 padding='same',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "model_convlstm.add(MaxPooling1D(pool_size=4))\n",
    "model_convlstm.add(LSTM(64, return_sequences=False))\n",
    "model_convlstm.add(Dense(32, activation='relu'))\n",
    "model_convlstm.add(Dense(actions.shape[0]))\n",
    "\n",
    "\n",
    "\n",
    "model_convlstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('Logs/Conv1d_LSTM_custom')\n",
    "tb_callback = TensorBoard(log_dir=log_dir,histogram_freq=1,\n",
    "                          update_freq='epoch',\n",
    "                          profile_batch=0) ## !tensorboard --logdir=.\n",
    "mc = ModelCheckpoint('Models/Conv1d_LSTM_custom.h5', monitor='val_categorical_accuracy', mode='max', verbose=1,save_best_only=True)\n",
    "es = EarlyStopping(monitor='val_categorical_accuracy', mode='max', verbose=1,patience=50)\n",
    "callbacks = [tb_callback,mc,es]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_convlstm.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_convlstm.fit(X_train, y_train, epochs=2000, callbacks=[callbacks],batch_size=4,validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 CONVLSTM1D TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (samples, time, rows, channels)\n",
    "X_train_reshaped = X_train.reshape([X_train.shape[0],X_train.shape[1],X_train.shape[2],1])\n",
    "X_test_reshaped = X_test.reshape([X_test.shape[0],X_train.shape[1],X_train.shape[2],1])\n",
    "X_train_reshaped.shape,X_test_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv1d_lstm_tf = Sequential()\n",
    "\n",
    "model_conv1d_lstm_tf.add(Input(shape=(X_train.shape[1],X_train.shape[2],1))) # time steps(frames) * features\n",
    "model_conv1d_lstm_tf.add(ConvLSTM1D(filters=64,kernel_size=(5), data_format='channels_last', padding = 'same'\n",
    "                      ,return_sequences=True))\n",
    "model_conv1d_lstm_tf.add(ConvLSTM1D(filters=32,kernel_size=(5), data_format='channels_last', padding = 'same'\n",
    "                      ,return_sequences=False))\n",
    "model_conv1d_lstm_tf.add(MaxPooling1D((32)))\n",
    "#model2.add(Dense(32, activation='relu'))\n",
    "model_conv1d_lstm_tf.add(TimeDistributed(Dense(actions.shape[0], activation='relu')))\n",
    "model_conv1d_lstm_tf.add(Flatten())\n",
    "model_conv1d_lstm_tf.add(Dense(actions.shape[0]))\n",
    "\n",
    "\n",
    "\n",
    "model_conv1d_lstm_tf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('Logs/Conv1d_LSTM_tf')\n",
    "tb_callback = TensorBoard(log_dir=log_dir,histogram_freq=1,\n",
    "                          update_freq='epoch',\n",
    "                          profile_batch=0) ## !tensorboard --logdir=.\n",
    "mc = ModelCheckpoint('Models/model_conv1d_lstm_tf.h5', monitor='val_categorical_accuracy', mode='max', verbose=1,save_best_only=True)\n",
    "es = EarlyStopping(monitor='val_categorical_accuracy', mode='max', verbose=1,patience=50)\n",
    "callbacks = [tb_callback,mc,es]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv1d_lstm_tf.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv1d_lstm_tf.fit(X_train_reshaped, y_train,callbacks=[callbacks], epochs=2000, batch_size=1,validation_data=(X_test_reshaped,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Conv1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv1d = Sequential()\n",
    "model_conv1d.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1],X_train.shape[2])))\n",
    "model_conv1d.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "model_conv1d.add(Dropout(0.5))\n",
    "model_conv1d.add(MaxPooling1D(pool_size=2))\n",
    "model_conv1d.add(Flatten())\n",
    "model_conv1d.add(Dense(100, activation='relu'))\n",
    "model_conv1d.add(Dense(actions.shape[0], activation='softmax'))\n",
    "model_conv1d.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv1d.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('Logs/Conv1d')\n",
    "tb_callback = TensorBoard(log_dir=log_dir,histogram_freq=1,\n",
    "                          update_freq='epoch',\n",
    "                          profile_batch=0) ## !tensorboard --logdir=.\n",
    "mc = ModelCheckpoint('Models/model_conv1d.h5', monitor='val_categorical_accuracy', mode='max', verbose=1,save_best_only=True)\n",
    "es = EarlyStopping(monitor='val_categorical_accuracy', mode='max', verbose=1,patience=50)\n",
    "callbacks = [tb_callback,mc,es]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv1d.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv1d.fit(X_train, y_train, epochs=2000, callbacks=[callbacks],batch_size=1,validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 With CTC Loss & Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTCLayer(layers.Layer):\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.loss_fn = keras.backend.ctc_batch_cost\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        # Compute the training-time loss value and add it\n",
    "        # to the layer using `self.add_loss()`.\n",
    "        batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
    "        input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
    "        label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
    "\n",
    "        input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "        label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "\n",
    "        loss = self.loss_fn(y_true, y_pred, input_length, label_length)\n",
    "        self.add_loss(loss)\n",
    "\n",
    "        # At test time, just return the computed predictions\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "\n",
    "    input_seq = layers.Input(\n",
    "            shape=(timesteps, features), name=\"seq\", dtype=\"float32\"\n",
    "        )\n",
    "    labels = layers.Input(name=\"label\", shape=(None,), dtype=\"int32\")\n",
    "\n",
    "    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True, dropout=0.25))(input_seq)\n",
    "    #x = layers.BatchNormalization(name=\"lstm_1_bn\")(x)\n",
    "    #x = layers.ReLU(name=\"lstm_1_relu\")(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(64, return_sequences=True, dropout=0.25))(x)\n",
    "    #x = layers.BatchNormalization(name=\"lstm_2_bn\")(x)\n",
    "    #x = layers.ReLU(name=\"lstm_2_relu\")(x)\n",
    "    #x = Dense(64, activation='relu')(x)\n",
    "\n",
    "    # Output layer\n",
    "    x = layers.Dense(actions.shape[0]+1, activation=\"softmax\", name=\"dense2\")(x)\n",
    "\n",
    "    # Add CTC layer for calculating CTC loss at each step\n",
    "    output = CTCLayer(name=\"ctc_loss\")(labels, x)\n",
    "\n",
    "    # Define the model\n",
    "    model = keras.models.Model(\n",
    "        inputs=[input_seq, labels], outputs=output, name=\"ctc_model_v1\"\n",
    "    )\n",
    "    # Optimizer\n",
    "    # Compile the model and return\n",
    "    model.compile(optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ctc_model = build_model()\n",
    "ctc_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(images, labels, train_size=0.9, shuffle=True):\n",
    "    # 1. Get the total size of the dataset\n",
    "    size = len(images)\n",
    "    # 2. Make an indices array and shuffle it, if required\n",
    "    indices = np.arange(size)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    # 3. Get the size of training samples\n",
    "    train_samples = int(size * train_size)\n",
    "    # 4. Split data into training and validation sets\n",
    "    x_train, y_train = images[indices[:train_samples]], labels[indices[:train_samples]]\n",
    "    x_valid, y_valid = images[indices[train_samples:]], labels[indices[train_samples:]]\n",
    "    return x_train, x_valid, y_train, y_valid\n",
    "\n",
    "\n",
    "# Splitting data into training and validation sets\n",
    "x_train, x_valid, y_train, y_valid = split_data(np.array(X2), np.array(labels1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_single_sample(seq, label):\n",
    "    return {\"seq\": seq, \"label\": label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = (\n",
    "    train_dataset.map(\n",
    "        encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    .batch(batch_size)\n",
    "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((x_valid, y_valid))\n",
    "validation_dataset = (\n",
    "    validation_dataset.map(\n",
    "        encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    .batch(batch_size)\n",
    "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "early_stopping_patience = 300\n",
    "# Add early stopping\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=early_stopping_patience, restore_best_weights=True\n",
    ")\n",
    "log_dir = os.path.join('Logs/CTC')\n",
    "tb_callback = TensorBoard(log_dir=log_dir) ## !tensorboard --logdir=.\n",
    "# Train the model\n",
    "history = ctc_model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=validation_dataset,\n",
    "    epochs=epochs,\n",
    "    callbacks=[early_stopping,tb_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_model = keras.models.Model(\n",
    "    ctc_model.get_layer(name=\"seq\").input, ctc_model.get_layer(name=\"dense2\").output\n",
    ")\n",
    "prediction_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A utility function to decode the output of the network\n",
    "def decode_batch_predictions(pred):\n",
    "    input_len = np.ones(pred.shape[0]) * pred.shape[1]\n",
    "    # Use greedy search. For complex tasks, you can use beam search\n",
    "    results = keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0][\n",
    "        :, :max_length\n",
    "    ]\n",
    "    prob = np.array(keras.backend.ctc_decode(pred, input_length=input_len, greedy=True,)[1][0][0]) #[0])#[:, :19])\n",
    "    # Iterate over the results and get back the text\n",
    "    output_text = []\n",
    "    for res in results:\n",
    "        #res = tf.strings.reduce_join(res).numpy().decode(\"utf-8\")\n",
    "        output_text.append(res)\n",
    "    return output_text , prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Let's check results on some validation samples\n",
    "predss=[]\n",
    "for batch in validation_dataset.take(10000):\n",
    "    batch_seq = batch[\"seq\"]\n",
    "    batch_labels = batch[\"label\"]\n",
    "    \n",
    "    preds = prediction_model.predict(batch_seq)\n",
    "    pred_texts = decode_batch_predictions(preds)[0]\n",
    "    predss.append((pred_texts == batch_labels)[0][0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(predss)/len(predss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6 Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Normalization and Attention\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(x, x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    return x + res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(\n",
    "    input_shape,\n",
    "    head_size,\n",
    "    num_heads,\n",
    "    ff_dim,\n",
    "    num_transformer_blocks,\n",
    "    mlp_units,\n",
    "    dropout=0,\n",
    "    mlp_dropout=0,\n",
    "):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(mlp_dropout)(x)\n",
    "    outputs = layers.Dense(actions.shape[0], activation=\"softmax\")(x)\n",
    "    return keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X_train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_transformer = build_model(\n",
    "    input_shape,\n",
    "    head_size=256,\n",
    "    num_heads=4,\n",
    "    ff_dim=4,\n",
    "    num_transformer_blocks=4,\n",
    "    mlp_units=[128],\n",
    "    mlp_dropout=0.4,\n",
    "    dropout=0.25,\n",
    ")\n",
    "model_transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_transformer.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=Adam(learning_rate=1e-5),\n",
    "    metrics=[\"categorical_accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('Logs/transformer_encoder')\n",
    "tb_callback = TensorBoard(log_dir=log_dir,histogram_freq=1,\n",
    "                          update_freq='epoch',\n",
    "                          profile_batch=0) ## !tensorboard --logdir=.\n",
    "mc = ModelCheckpoint('Models/model_transformer_encoder.h5', monitor='val_categorical_accuracy', mode='max', verbose=1,save_best_only=True)\n",
    "es = EarlyStopping(monitor='val_categorical_accuracy', mode='max', verbose=1,patience=100)\n",
    "callbacks = [tb_callback,mc,es]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_transformer.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=2000,\n",
    "    batch_size=8,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_transformer.evaluate(X_test, y_test, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(model_transformer.predict(np.expand_dims(X_test[87],axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(y_test[87])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.predict(X_test)\n",
    "np.argmax(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions[np.argmax(res[4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions[np.argmax(y_test[4])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Save and Load Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('action.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model_transformer_encoder.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Evaluation using Confusion Matrix and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model_lstm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "yhat = np.argmax(yhat, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.heatmap(confusion_matrix(ytrue, yhat),annot =True,yticklabels=actions,xticklabels=actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(ytrue, yhat)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee184bebc06f3122aeb5a2069a23f71eb4630874e570a919845476df54908f4"
  },
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
